import os
from dotenv import load_dotenv
from langchain.schema import Document
from elasticsearch import Elasticsearch
from langchain_elasticsearch import ElasticsearchStore

from processor1_pdf2md import Parser2Markdown
from processor2_chunking import HeaderSplitter, SemanticSplitter
from processor3_embedding import BgeM3Embedding

load_dotenv()
#os.chdir('../experiment_results_upstage/') # ì´ê±´ ì‹œí—˜ìš© (ì´ë¯¸ íŒŒì‹±ëœ HTML ë¬¸ì„œ í´ë”)
os.chdir('../data/')

# ë³€ìˆ˜ ìƒì„±
UPSTAGE_API_KEY = os.getenv("UPSTAGE_API_KEY") 
preprocessor = Parser2Markdown(UPSTAGE_API_KEY)

embeddings = BgeM3Embedding()
header_splitter = HeaderSplitter()
second_splitter = SemanticSplitter(embeddings)
es =  Elasticsearch('http://localhost:9200')


vectorstore = ElasticsearchStore(
    index_name="test-0524",
    embedding=embeddings,
    es_connection=es,
)

    
# # ì´ë¯¸ íŒŒì‹±ëœ íŒŒì¼ ì½ì–´ì˜¤ëŠ” ê²½ìš°
# def read_md_file(file_path):
#     with open(file_path, 'r', encoding='utf-8') as f:
#         return f.read()


for file_name in os.listdir('.'):
    ## file_name ì˜ˆ) 0000060830_20250508_(250414)í–‰ì •ì¤‘ì‹¬ë³µí•©ë„ì‹œ6-3M2ë¸”ë¡ì…ì£¼ìëª¨ì§‘ê³µê³ ë¬¸(ì •ì •-ì ‘ìˆ˜ê¸°ê°„ì—°ì¥).pdf
    print(f"======== ğŸš© {file_name} íŒŒì¼ì— ëŒ€í•œ ì²˜ë¦¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤. ========")
    
    ## 1. ê¸°ì¡´ ë¬¸ì„œ ì‚­ì œ (Elasticsearch ì¿¼ë¦¬)
    apt_code = file_name.split('_')[0] # file_name
    delete_query = {
        "query": {
            "term": {
                "metadata.apt_code": apt_code
            }
        }
    }
    es.delete_by_query(index="test-0524", body=delete_query)
    print(f"â–¶INFO: ğŸ—‘ï¸ ê¸°ì¡´ apt_code={apt_code} ë¬¸ì„œê°€ Elasticsearchì—ì„œ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    
    ## 2. PDFë¬¸ íŒŒì‹± ë° ë§ˆí¬ë‹¤ìš´ í˜•íƒœë¡œ ë³€í™˜  
    #html_contents = preprocessor.pdf_upstageparser(file_name)
    html_contents = preprocessor.pdf_openparse(file_name)
    markdown_texts = preprocessor.html2md_with_spans(html_contents) 

    doc = Document(
        page_content=markdown_texts,
        metadata={"source_pdf": file_name}
    )
    
    ## 3. 1ì°¨ í—¤ë” ê¸°ë°˜ ì²­í¬
    header_chunks = header_splitter.split_documents([doc])

    ## 4. 2ì°¨ ì˜ë¯¸ ê¸°ë°˜ ì²­í¬
    documents = second_splitter.split_documents(header_chunks)

    ## 5. ì¼ê´„ ì„ë² ë”© + ë²¡í„° ì €ì¥
    vectorstore.add_documents(documents)
    print(f"-Ë‹Ëâœ„â”ˆâ”ˆâ”ˆâ”ˆâ”ˆâ”ˆâ”ˆâ”ˆâ”ˆâ”ˆâ”ˆâ”ˆ [ì™„ë£Œ] {len(documents)}ê°œ ë¬¸ì„œ Elasticsearchì— ì ì¬ë˜ì—ˆìŠµë‹ˆë‹¤. â”ˆâ”ˆâ”ˆâ”ˆâ”ˆâ”ˆâ”ˆâ”ˆâ”ˆâ”ˆâ”ˆâ”ˆ\n\n")