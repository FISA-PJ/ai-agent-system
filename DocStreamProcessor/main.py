import os
from typing import List
from dotenv import load_dotenv
from langchain.schema import Document
from elasticsearch import Elasticsearch, NotFoundError, TransportError
from langchain_elasticsearch import ElasticsearchStore

from DocStreamProcessor import Parser2Markdown, HeaderSplitter, SemanticSplitter, BgeM3Embedding


# ===== ê¸°ë³¸ ì„¤ì • =====
load_dotenv()
DATA_DIR = "experiment_results_upstage"
INDEX_NAME = "test-0524"
UPSTAGE_API_KEY = os.getenv("UPSTAGE_API_KEY")

# ===== ì¸ìŠ¤í„´ìŠ¤ ì´ˆê¸°í™” =====
preprocessor = Parser2Markdown(UPSTAGE_API_KEY)
embeddings = BgeM3Embedding()
header_splitter = HeaderSplitter()
semantic_splitter = SemanticSplitter(embeddings)
es =  Elasticsearch('http://localhost:9200')
vectorstore = ElasticsearchStore(
    index_name=INDEX_NAME,
    embedding=embeddings,
    es_connection=es,
)

    
# ===== utils =====
def read_md_file(file_path):
    with open(file_path, 'r', encoding='utf-8') as f:
        return f.read()


def delete_documents_by_apt_code(es: Elasticsearch, index: str, apt_code: str):
    if not es.indices.exists(index=index):
        print(f"âš ï¸ ì¸ë±ìŠ¤ '{index}'ê°€ ì¡´ì¬í•˜ì§€ ì•Šì•„ ì‚­ì œë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        return
    
    query = {"query": {"term": {"metadata.apt_code": apt_code}}}
    
    try :
        response = es.delete_by_query(index = index, body = query)
        deleted = response.get(deleted, 0)
        print(f"ğŸ—‘ï¸ apt_code={apt_code} ë¬¸ì„œ {deleted}ê±´ ì‚­ì œ ì™„ë£Œ" if deleted else f"â„¹ï¸ apt_code={apt_code} ê´€ë ¨ ë¬¸ì„œ ì—†ìŒ")
        
    except NotFoundError as e:
        print(f"ğŸš« ë¬¸ì„œ ì—†ìŒ: {e}")
    except TransportError as e:
        print(f"ğŸš¨ ìš”ì²­ ì‹¤íŒ¨: {e.error}, ìƒíƒœ ì½”ë“œ: {e.status_code}")
    except Exception as e:
        print(f"â—ì˜ˆì™¸ ë°œìƒ: {e}")
        

def doc2es(file_name) :
    print(f"======== ğŸš© {file_name} íŒŒì¼ì— ëŒ€í•œ ì²˜ë¦¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤. ========")
    
    ## 1. ê¸°ì¡´ ë¬¸ì„œ ì‚­ì œ (Elasticsearch ì¿¼ë¦¬)
    apt_code = file_name.split('_')[0]
    delete_documents_by_apt_code(apt_code)

    ## 2. PDFë¬¸ íŒŒì‹± ë° ë§ˆí¬ë‹¤ìš´ í˜•íƒœë¡œ ë³€í™˜  
    #html_contents = preprocessor.pdf_upstageparser(file_name)
    #html_contents = preprocessor.pdf_openparse(file_name)
    html_contents = read_md_file(os.path.join(DATA_DIR, file_name))
    markdown_texts = preprocessor.html_to_markdown_with_tables(html_contents) 

    ## 3. Chunking 
    doc = Document(page_content=markdown_texts, metadata={"source_pdf": file_name})
    header_chunks = header_splitter.split_documents([doc])
    final_documents = semantic_splitter.split_documents(header_chunks)
    
    ## 4. ì¼ê´„ ì„ë² ë”© + ë²¡í„° ì €ì¥
    vectorstore.add_documents(final_documents)
    print(f"-Ë‹Ëâœ„ [ì™„ë£Œ] {len(final_documents)}ê°œ ë¬¸ì„œ Elasticsearchì— ì ì¬ë˜ì—ˆìŠµë‹ˆë‹¤.\n")


if __name__ == '__main__' :
    os.chdir(DATA_DIR)

    for file_name in os.listdir("."):
        doc2es(file_name)